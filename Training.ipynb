{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6401b4bc-f316-4e94-9661-14c37828b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tifffile\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as tvf\n",
    "from torchmetrics import PeakSignalNoiseRatio\n",
    "\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c121261d-0ee2-452c-87f0-6877ceca0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2K_Data(Dataset):\n",
    "    def __init__(self, csv_path, is_transform=False):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.is_transform = is_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get_difference(self, tensor_image_1, tensor_image_2):\n",
    "        image_1 = tensor_image_1.detach().numpy()\n",
    "        image_2 = tensor_image_2.detach().numpy()\n",
    "\n",
    "        difference = image_1 - image_2\n",
    "\n",
    "        return torch.from_numpy(difference)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        lr_image = read_image(self.data.iloc[index, 2])\n",
    "        lr_height, lr_width = tvf.get_image_size(lr_image)\n",
    "        resize = transforms.Resize((lr_width*2, lr_height*2))\n",
    "        \n",
    "        # Normalization using ImageNet measures of center and spread.\n",
    "        normalize = transforms.Normalize(mean=torch.Tensor([0.485, 0.456, 0.406]), \n",
    "                                          std=torch.Tensor([0.229, 0.224, 0.225]), \n",
    "                                          inplace=True)\n",
    "        # tensorify = transforms.ToTensor()\n",
    "        lr_interpolated_image = resize(lr_image)\n",
    "        hr_image = read_image(self.data.iloc[index, 5])\n",
    "        if self.is_transform:\n",
    "            if random.random() > 0.5:\n",
    "                angle = random.randint(0, 180)\n",
    "                lr_interpolated_image = tvf.rotate(lr_interpolated_image, angle)\n",
    "                hr_image = tvf.rotate(hr_image, angle)\n",
    "                \n",
    "            if random.random() > 0.5:\n",
    "                lr_interpolated_image = tvf.hflip(lr_interpolated_image)\n",
    "                hr_image = tvf.hflip(hr_image)\n",
    "            \n",
    "            if random.random() > 0.5:\n",
    "                lr_interpolated_image = tvf.vflip(lr_interpolated_image)\n",
    "                hr_image = tvf.vflip(hr_image)\n",
    "        \n",
    "        lr_interpolated_image = normalize(lr_interpolated_image.type(torch.float32))\n",
    "        hr_image = normalize(hr_image.type(torch.float32))\n",
    "        return lr_interpolated_image, hr_image, self.get_difference(hr_image, lr_interpolated_image).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf8048b-0f24-4908-9379-ead481645e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html\n",
    "def plot_multiple_images(imgs, title):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(18, 8))\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = tvf.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(title=title)\n",
    "        # axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b3682a-0554-402f-94ec-9417d0efe7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_number_of_images(data_torch, n_rand_images = 5):\n",
    "    # Show n random images from the training set\n",
    "    rand_indices = [random.randint(0, len(data_torch)) for _ in range(n_rand_images)]\n",
    "\n",
    "    for i in rand_indices:\n",
    "        sample_lr, sample_hr, residual_diff = data_torch[i]\n",
    "        print(i, sample_lr.shape, sample_hr.shape, residual_diff.shape)\n",
    "        grid = make_grid([sample_lr, sample_hr, residual_diff])\n",
    "        plot_multiple_images(grid, \"Low Resolution image - \" + \"High Resolution image - \" + \"Difference in images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "087b4ac6-1457-4a03-8d3e-a3b0d86920a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    TRAIN_PATH = \"./DIV2K_train_subset.csv\"\n",
    "    VALID_PATH = \"./DIV2K_valid_subset.csv\"\n",
    "    MODEL_SAVEPATH = \"./models/\" #path to save models\n",
    "    BATCH_SIZE = 2 #batch size\n",
    "    INPUT_CHANNELS = 3\n",
    "    OUTPUT_CHANNELS = 3\n",
    "    DEPTH = 6\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    EPOCHS = 80\n",
    "    LOSS_MSE = nn.MSELoss()\n",
    "    LOSS_STR = \"MSE Loss\"\n",
    "    EVALUATION_METRIC = PeakSignalNoiseRatio()\n",
    "    EVALUATION_STR = \"Peak Signal To Noise Ratio (PSNR)\"\n",
    "    GRAD_CLIP_MAX_NORM = 1\n",
    "    MOMENTUM = 0.9\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    LEARNING_RATE = 0.1\n",
    "    OPTIM_STEP_SIZE = 20\n",
    "    OPTIM_GAMMA = 0.1\n",
    "    ARCHITECTURE = \"VDSR-Net\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        details = \"\"\n",
    "        details += \"-\"*40 + \"CONFIGURATION DETAILS\" + \"-\"*40 + \"\\n\"\n",
    "        details += f\"Architecture : {self.ARCHITECTURE}\\n\"\n",
    "        details += f\"Batch Size : {str(self.BATCH_SIZE)}\\n\"\n",
    "        details += f\"Number of Input Channels : {str(self.INPUT_CHANNELS)}\\n\"\n",
    "        details += f\"Number of Output Channels : {str(self.OUTPUT_CHANNELS)}\\n\"\n",
    "        details += f\"Depth of the network : {str(self.DEPTH)}\\n\"\n",
    "        details += f\"Training platform : {self.DEVICE.upper()}\\n\"\n",
    "        details += f\"Number of epochs : {str(self.EPOCHS)}\\n\"\n",
    "        details += f\"Gradient clipping with max norm : {str(self.GRAD_CLIP_MAX_NORM)}\\n\"\n",
    "        details += f\"Loss Function : {self.LOSS_STR}\\n\"\n",
    "        details += f\"Performance metric : {self.EVALUATION_STR}\\n\"\n",
    "        details += \"-\"*100\n",
    "        return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f6102d-a1e6-4873-8977-502cd059d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, train_dataloader, valid_dataloader, config):\n",
    "        self.patience = 5\n",
    "        self.config = config\n",
    "        self.model = VDSR_Net(in_channels=self.config.INPUT_CHANNELS, \n",
    "                              out_channels=self.config.OUTPUT_CHANNELS, \n",
    "                              depth=self.config.DEPTH)\n",
    "        self.loss_function = self.config.LOSS_MSE\n",
    "        self.batch_size = self.config.BATCH_SIZE\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.device = self.config.DEVICE\n",
    "        self.epochs = self.config.EPOCHS\n",
    "        self.lr = self.config.LEARNING_RATE\n",
    "        self.optim_step_size = self.config.OPTIM_STEP_SIZE\n",
    "        self.optim_gamma = self.config.OPTIM_GAMMA\n",
    "        self.grad_clip_max_norm = self.config.GRAD_CLIP_MAX_NORM\n",
    "        self.momentum=self.config.MOMENTUM\n",
    "        self.weight_decay=self.config.WEIGHT_DECAY\n",
    "        self.evaluation_metric = self.config.EVALUATION_METRIC\n",
    "        self.device = self.config.DEVICE\n",
    "        self.val_for_early_stopping = 9999999 #early stopping\n",
    "        \n",
    "        if not os.path.isdir(self.config.MODEL_SAVEPATH):\n",
    "            os.makedirs(self.config.MODEL_SAVEPATH)\n",
    "        \n",
    "        self.log = pd.DataFrame(columns=[\"model_name\", \"train_loss\", \"train_PSNR\", \"valid_loss\", \"valid_PSNR\"])\n",
    "        self.optimizer = optim.SGD(params=self.model.parameters(), \n",
    "                                   lr=self.lr, \n",
    "                                   momentum=self.momentum,\n",
    "                                   weight_decay=self.weight_decay)\n",
    "        self.optim_scheduler = optim.lr_scheduler.StepLR(self.optimizer, \n",
    "                                      step_size=self.optim_step_size,\n",
    "                                      gamma=self.optim_gamma)\n",
    "    \n",
    "    def calculate_metrics(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_performance = 0\n",
    "        with torch.no_grad():\n",
    "            for lr_image, _, res_diff in tqdm(dataloader, total=len(dataloader)):\n",
    "                low_res_image = lr_image.to(self.device)\n",
    "                residual_diff = res_diff.to(self.device)\n",
    "                out = self.model(low_res_image)\n",
    "                loss = self.loss_function(out.data, residual_diff)\n",
    "                total_loss += loss.item()\n",
    "                total_performance += self.evaluation_metric(out.data.to(\"cpu\"), residual_diff.cpu())\n",
    "        return total_performance/len(dataloader), total_loss/len(dataloader)\n",
    "    \n",
    "    \n",
    "    def early_stopping(self, val_loss):\n",
    "        if val_loss < self.val_for_early_stopping:\n",
    "            self.val_for_early_stopping = val_loss\n",
    "            return True\n",
    "        else:\n",
    "            self.patience -= 1\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        print(\"-\"*25, \"THE MODEL BASED ON\" , self.config.ARCHITECTURE, \"BEGINS TRAINING\", \"-\"*25)\n",
    "        print(f\"TRAINING ON {self.config.DEVICE.upper()}\")\n",
    "        best_loss = 9e+6\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            self.model.to(self.device)\n",
    "            \n",
    "            performance_train = 0\n",
    "            loss_train = 0\n",
    "            \n",
    "            for lr_image, _, res_diff in tqdm(self.train_dataloader, total=len(self.train_dataloader)):\n",
    "                \n",
    "                low_res_batch = lr_image.to(self.device)\n",
    "                # print(low_res_batch.dtype)\n",
    "                residual_diff_batch = res_diff.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(low_res_batch)\n",
    "                loss = self.loss_function(output, residual_diff_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip_max_norm)\n",
    "                loss_train += loss.item()\n",
    "                performance_train += self.evaluation_metric(output.data.to(\"cpu\"), residual_diff_batch.to(\"cpu\"))\n",
    "            \n",
    "            self.optim_scheduler.step()\n",
    "            # print(f\"LOSS: {loss_train}\")\n",
    "            # print(f\"Performance: {performance_train}\")\n",
    "            \n",
    "            performance_valid, loss_valid = self.calculate_metrics(self.valid_dataloader)\n",
    "            performance_train /= len(self.train_dataloader)\n",
    "            loss_train /= len(self.train_dataloader)\n",
    "            \n",
    "            print(\"-\"*10, \"STATUS AT EPOCH NO.\", epoch, \"-\"*10)\n",
    "            print(f\"Train performance : {performance_train}, Train loss {loss_train}\")\n",
    "            print(f\"Valid performance : {performance_valid}, valid loss {loss_valid}\")\n",
    "            \n",
    "            self.log.loc[epoch,:] = [f\"{self.config.ARCHITECTURE}_{self.config.BATCH_SIZE}.pth\", \n",
    "                                     f\"{loss_train}\",\n",
    "                                     f\"{performance_train}\",\n",
    "                                     f\"{loss_valid}\",\n",
    "                                     f\"{performance_valid}\"]\n",
    "            self.log.to_csv(self.config.MODEL_SAVEPATH + \n",
    "                            f\"/{self.config.ARCHITECTURE}_{self.config.BATCH_SIZE}_valid_{epoch}.csv\",index=False)\n",
    "            \n",
    "            if self.patience >= 0 and self.early_stopping(loss_valid):\n",
    "                print(f\"Saving model at Epoch: {epoch}\")\n",
    "                torch.save(self.model.state_dict(), self.config.MODEL_SAVEPATH + \"/\" +\n",
    "                           f\"{self.config.ARCHITECTURE}_{self.config.BATCH_SIZE}.pth\")\n",
    "                self.patience = 5\n",
    "            \n",
    "            if self.patience <= 0:\n",
    "                print(\"-\"*10, \"EARLY STOPPING\", \"-\"*10)\n",
    "                print(\"Training terminated, no improvement in valid loss\")\n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfee6512-9c5b-49cb-99f8-34414313327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config, n_images_to_viz=0):\n",
    "    train_data = DIV2K_Data(csv_path=config.TRAIN_PATH, is_transform=True)\n",
    "    valid_data = DIV2K_Data(csv_path=config.VALID_PATH, is_transform=False)\n",
    "    if(n_images_to_viz):\n",
    "        visualize_random_number_of_images(train_data, n_images_to_viz)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=vdsr_config.BATCH_SIZE, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_data, batch_size=vdsr_config.BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    trainer = Trainer(train_dataloader, valid_dataloader, config)\n",
    "    trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3f2235-3338-4d8a-8708-61d104c170e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------CONFIGURATION DETAILS----------------------------------------\n",
      "Architecture : VDSR-Net\n",
      "Batch Size : 2\n",
      "Number of Input Channels : 3\n",
      "Number of Output Channels : 3\n",
      "Depth of the network : 6\n",
      "Training platform : CUDA\n",
      "Number of epochs : 80\n",
      "Gradient clipping with max norm : 1\n",
      "Loss Function : MSE Loss\n",
      "Performance metric : Peak Signal To Noise Ratio (PSNR)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vdsr_config = Configuration()\n",
    "print(vdsr_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a102f-deba-4e75-8200-42a8b15e61cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- THE MODEL BASED ON VDSR-Net BEGINS TRAINING -------------------------\n",
      "TRAINING ON CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [13:38<00:00,  3.81s/it]\n",
      "100%|██████████| 27/27 [00:37<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- STATUS AT EPOCH NO. 0 ----------\n",
      "Train performance : nan, Train loss nan\n",
      "Valid performance : nan, valid loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 78/215 [04:57<08:50,  3.88s/it]"
     ]
    }
   ],
   "source": [
    "main(vdsr_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4592e2-0a27-4ff7-82ae-8f9668336f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
