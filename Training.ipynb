{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6401b4bc-f316-4e94-9661-14c37828b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tifffile\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as tvf\n",
    "from torchmetrics import PeakSignalNoiseRatio\n",
    "\n",
    "from model import *\n",
    "from model_reference import *\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c121261d-0ee2-452c-87f0-6877ceca0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2K_Data(Dataset):\n",
    "    def __init__(self, csv_path, is_transform=False):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.is_transform = is_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get_difference(self, tensor_image_1, tensor_image_2):\n",
    "        image_1 = tensor_image_1.detach().numpy()\n",
    "        image_2 = tensor_image_2.detach().numpy()\n",
    "\n",
    "        difference = image_1 - image_2\n",
    "\n",
    "        return torch.from_numpy(difference)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        lr_image = read_image(self.data.iloc[index, 2])\n",
    "        lr_height, lr_width = tvf.get_image_size(lr_image)\n",
    "        resize = transforms.Resize((lr_width*2, lr_height*2))\n",
    "        \n",
    "        # Normalization using ImageNet measures of center and spread.\n",
    "        normalize = transforms.Normalize(mean=torch.Tensor([0.485, 0.456, 0.406]), \n",
    "                                          std=torch.Tensor([0.229, 0.224, 0.225]), \n",
    "                                          inplace=True)\n",
    "        # tensorify = transforms.ToTensor()\n",
    "        lr_interpolated_image = resize(lr_image)\n",
    "        hr_image = read_image(self.data.iloc[index, 5])\n",
    "        if self.is_transform:\n",
    "            if random.random() > 0.5:\n",
    "                angle = random.randint(0, 180)\n",
    "                lr_interpolated_image = tvf.rotate(lr_interpolated_image, angle)\n",
    "                hr_image = tvf.rotate(hr_image, angle)\n",
    "                \n",
    "            if random.random() > 0.5:\n",
    "                lr_interpolated_image = tvf.hflip(lr_interpolated_image)\n",
    "                hr_image = tvf.hflip(hr_image)\n",
    "            \n",
    "            if random.random() > 0.5:\n",
    "                lr_interpolated_image = tvf.vflip(lr_interpolated_image)\n",
    "                hr_image = tvf.vflip(hr_image)\n",
    "        \n",
    "        lr_interpolated_image = normalize(lr_interpolated_image.type(torch.float32))\n",
    "        hr_image = normalize(hr_image.type(torch.float32))\n",
    "        return lr_interpolated_image, hr_image, self.get_difference(hr_image, lr_interpolated_image).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf8048b-0f24-4908-9379-ead481645e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html\n",
    "def plot_multiple_images(imgs, title):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(18, 8))\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = tvf.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(title=title)\n",
    "        # axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b3682a-0554-402f-94ec-9417d0efe7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_number_of_images(data_torch, n_rand_images = 5):\n",
    "    # Show n random images from the training set\n",
    "    rand_indices = [random.randint(0, len(data_torch)) for _ in range(n_rand_images)]\n",
    "\n",
    "    for i in rand_indices:\n",
    "        sample_lr, sample_hr, residual_diff = data_torch[i]\n",
    "        print(i, sample_lr.shape, sample_hr.shape, residual_diff.shape)\n",
    "        grid = make_grid([sample_lr, sample_hr, residual_diff])\n",
    "        plot_multiple_images(grid, \"Low Resolution image - \" + \"High Resolution image - \" + \"Difference in images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f6102d-a1e6-4873-8977-502cd059d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, train_dataloader, valid_dataloader, config):\n",
    "        self.patience = 5\n",
    "        self.config = config\n",
    "        self.model = VDSR(in_channels=self.config.INPUT_CHANNELS, \n",
    "                              out_channels=self.config.OUTPUT_CHANNELS, )\n",
    "        self.loss_function = self.config.LOSS_MSE\n",
    "        self.batch_size = self.config.BATCH_SIZE\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.device = self.config.DEVICE\n",
    "        self.epochs = self.config.EPOCHS\n",
    "        self.lr = self.config.LEARNING_RATE\n",
    "        self.optim_step_size = self.config.OPTIM_STEP_SIZE\n",
    "        self.optim_gamma = self.config.OPTIM_GAMMA\n",
    "        self.grad_clip_max_norm = self.config.GRAD_CLIP_MAX_NORM\n",
    "        self.momentum=self.config.MOMENTUM\n",
    "        self.weight_decay=self.config.WEIGHT_DECAY\n",
    "        self.evaluation_metric = self.config.EVALUATION_METRIC\n",
    "        self.device = self.config.DEVICE\n",
    "        self.val_for_early_stopping = 9999999 #early stopping\n",
    "        \n",
    "        if not os.path.isdir(self.config.MODEL_SAVEPATH):\n",
    "            os.makedirs(self.config.MODEL_SAVEPATH)\n",
    "        \n",
    "        self.log = pd.DataFrame(columns=[\"model_name\", \"train_loss\", \"train_PSNR\", \"valid_loss\", \"valid_PSNR\"])\n",
    "        self.optimizer = optim.SGD(params=self.model.parameters(), \n",
    "                                   lr=self.lr, \n",
    "                                   momentum=self.momentum,\n",
    "                                   weight_decay=self.weight_decay)\n",
    "        self.optim_scheduler = optim.lr_scheduler.StepLR(self.optimizer, \n",
    "                                      step_size=self.optim_step_size,\n",
    "                                      gamma=self.optim_gamma)\n",
    "    \n",
    "    def calculate_metrics(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_performance = 0\n",
    "        with torch.no_grad():\n",
    "            for lr_image, hr_image, _ in tqdm(dataloader, total=len(dataloader)):\n",
    "                low_res_image = lr_image.to(self.device)\n",
    "                high_res_image = hr_image.to(self.device)\n",
    "                out = self.model(low_res_image)\n",
    "                loss = self.loss_function(out.data, high_res_image)\n",
    "                total_loss += loss.item()\n",
    "                total_performance += self.evaluation_metric(out.data.to(\"cpu\"), high_res_image.cpu())\n",
    "        return total_performance/len(dataloader), total_loss/len(dataloader)\n",
    "    \n",
    "    \n",
    "    def early_stopping(self, val_loss):\n",
    "        if val_loss < self.val_for_early_stopping:\n",
    "            self.val_for_early_stopping = val_loss\n",
    "            return True\n",
    "        else:\n",
    "            self.patience -= 1\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        print(\"-\"*25, \"THE MODEL BASED ON\" , self.config.ARCHITECTURE, \"BEGINS TRAINING\", \"-\"*25)\n",
    "        print(f\"TRAINING ON {self.config.DEVICE.upper()}\")\n",
    "        \n",
    "        best_loss = 9e+6\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            self.model.to(self.device)\n",
    "            log_file = open(f\"log_epoch_{epoch}.txt\", \"a\")\n",
    "            log_messages = \"\"\n",
    "            \n",
    "            performance_train = 0\n",
    "            loss_train = 0\n",
    "            b_num = 0\n",
    "            \n",
    "            for lr_image, hr_image, res_diff in tqdm(self.train_dataloader, total=len(self.train_dataloader)):\n",
    "                \n",
    "                low_res_batch = lr_image.to(self.device)\n",
    "                # print(low_res_batch.dtype)\n",
    "                high_res_image = hr_image.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(low_res_batch)\n",
    "                loss = self.loss_function(output, high_res_image)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip_max_norm)\n",
    "                loss_train += loss.item()\n",
    "                performance_train += self.evaluation_metric(output.data.to(\"cpu\"), hr_image.to(\"cpu\"))\n",
    "                log_messages += f\"LOSS, Performance at batch {b_num} after epoch {epoch}: {loss_train} : {performance_train}\\n\"\n",
    "                b_num += 1\n",
    "            \n",
    "            self.optim_scheduler.step()\n",
    "            \n",
    "            log_file.write(log_messages)\n",
    "            log_file.close()\n",
    "            \n",
    "            performance_valid, loss_valid = self.calculate_metrics(self.valid_dataloader)\n",
    "            performance_train /= len(self.train_dataloader)\n",
    "            loss_train /= len(self.train_dataloader)\n",
    "            \n",
    "            print(\"-\"*10, \"STATUS AT EPOCH NO.\", epoch, \"-\"*10)\n",
    "            print(f\"Train performance : {performance_train}, Train loss {loss_train}\")\n",
    "            print(f\"Valid performance : {performance_valid}, valid loss {loss_valid}\")\n",
    "            \n",
    "            self.log.loc[epoch,:] = [f\"{self.config.ARCHITECTURE}_{self.config.BATCH_SIZE}.pth\", \n",
    "                                     f\"{loss_train}\",\n",
    "                                     f\"{performance_train}\",\n",
    "                                     f\"{loss_valid}\",\n",
    "                                     f\"{performance_valid}\"]\n",
    "            self.log.to_csv(self.config.MODEL_SAVEPATH + \n",
    "                            f\"/{self.config.ARCHITECTURE}_{self.config.BATCH_SIZE}_valid_{epoch}.csv\",index=False)\n",
    "            \n",
    "            if self.patience >= 0 and self.early_stopping(loss_valid):\n",
    "                print(f\"Saving model at Epoch: {epoch}\")\n",
    "                torch.save(self.model.state_dict(), self.config.MODEL_SAVEPATH + \"/\" +\n",
    "                           f\"{self.config.ARCHITECTURE}_{self.config.BATCH_SIZE}.pth\")\n",
    "                self.patience = 5\n",
    "            \n",
    "            if self.patience <= 0:\n",
    "                print(\"-\"*10, \"EARLY STOPPING\", \"-\"*10)\n",
    "                print(\"Training terminated, no improvement in valid loss\")\n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfee6512-9c5b-49cb-99f8-34414313327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config, n_images_to_viz=0):\n",
    "    train_data = DIV2K_Data(csv_path=config.TRAIN_PATH, is_transform=True)\n",
    "    valid_data = DIV2K_Data(csv_path=config.VALID_PATH, is_transform=False)\n",
    "    if(n_images_to_viz):\n",
    "        visualize_random_number_of_images(train_data, n_images_to_viz)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=vdsr_config.BATCH_SIZE, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_data, batch_size=vdsr_config.BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    trainer = Trainer(train_dataloader, valid_dataloader, config)\n",
    "    trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a3f2235-3338-4d8a-8708-61d104c170e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------CONFIGURATION DETAILS----------------------------------------\n",
      "Architecture : VDSR-Net\n",
      "Batch Size : 2\n",
      "Number of Input Channels : 3\n",
      "Number of Output Channels : 3\n",
      "Depth of the network : 6\n",
      "Training platform : CUDA\n",
      "Number of epochs : 80\n",
      "Gradient clipping with max norm : 0.01\n",
      "Loss Function : MSE Loss\n",
      "Performance metric : Peak Signal To Noise Ratio (PSNR)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vdsr_config = Configuration()\n",
    "print(vdsr_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b1a102f-deba-4e75-8200-42a8b15e61cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- THE MODEL BASED ON VDSR-Net BEGINS TRAINING -------------------------\n",
      "TRAINING ON CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [09:37<00:00,  2.69s/it]\n",
      "100%|██████████| 27/27 [00:46<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- STATUS AT EPOCH NO. 0 ----------\n",
      "Train performance : nan, Train loss nan\n",
      "Valid performance : nan, valid loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 84/215 [02:41<04:12,  1.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d0cf5b812cd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvdsr_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-660f099d6035>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config, n_images_to_viz)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e6fe07af8b68>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_clip_max_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mloss_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mperformance_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mlog_messages\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"LOSS, Performance at batch {b_num} after epoch {epoch}: {loss_train} : {performance_train}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(vdsr_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4592e2-0a27-4ff7-82ae-8f9668336f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
